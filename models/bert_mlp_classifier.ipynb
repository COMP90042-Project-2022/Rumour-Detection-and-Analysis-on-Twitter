{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_mlp_classifier.ipynb","provenance":[{"file_id":"1gNqjJKXqPbI4DyUoBM74F6XaSlYCOAyZ","timestamp":1651431954978}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyORS68VzXNQ37KEL/WM/gQa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install emoji\n","# !pip install cloud-tpu-client==0.10 torch==1.10.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezQKIbJPA1wf","executionInfo":{"status":"ok","timestamp":1651997843120,"user_tz":-600,"elapsed":6225,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}},"outputId":"7d196a3a-45fb-4459-ec48-cc360041e21f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZopoAAFhQHOk","executionInfo":{"status":"ok","timestamp":1651997847278,"user_tz":-600,"elapsed":4170,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"outputs":[],"source":["import re\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel, BertConfig\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","\n","import copy\n","\n","# import torch_xla\n","# import torch_xla.core.xla_model as xm\n","# import torch_xla.distributed.parallel_loader as pl\n","# import torch_xla.distributed.xla_multiprocessing as xmp\n","# import torch_xla.utils.utils as xu"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# torch.cuda.empty_cache()"],"metadata":{"id":"4zlM6GLeKIKJ","executionInfo":{"status":"ok","timestamp":1651997847279,"user_tz":-600,"elapsed":33,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7oQCBpU2PX0","executionInfo":{"status":"ok","timestamp":1651997847280,"user_tz":-600,"elapsed":29,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}},"outputId":"fe3e15a7-9ed6-479b-c7ec-3c59371fdf1e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun May  8 08:18:25 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P0    29W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["print(torch.cuda.memory_allocated())\n","print(torch.cuda.memory_reserved())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQCtbDf23LLi","executionInfo":{"status":"ok","timestamp":1651997847282,"user_tz":-600,"elapsed":22,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}},"outputId":"169f0dac-ebd8-4250-c1dc-e12b75f8540b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n"]}]},{"cell_type":"code","source":["bert_model = \"vinai/bertweet-base\"\n","# bert_model = 'bert-base-uncased'\n","# bert_model = 'bert-large-uncased'\n","# bert_model = 'google/electra-small-discriminator'\n","# bert_model = \"roberta-base\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","bert = AutoModel.from_pretrained(bert_model)"],"metadata":{"id":"ap_4pQcUwl27","executionInfo":{"status":"ok","timestamp":1651997853936,"user_tz":-600,"elapsed":6671,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d6e2d49-29da-4c8a-9140-ee9436040a78"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["class TweetDataset(Dataset):\n","\n","    def __init__(self, path, tokenizer=tokenizer, is_test=False):\n","\n","        self.df = pd.read_csv(path, delimiter = '\\t')\n","        self.tokenizer = tokenizer\n","        self.is_test = is_test\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        \n","        tweets = self.df.loc[index, 'text']\n","        \n","        tweets = self.preprocess(tweets)\n","        inputs = self.tokenizer(tweets, padding='max_length', truncation=True, return_tensors=\"pt\")\n","        \n","        input_ids = inputs['input_ids'][0]\n","        attention_mask = inputs['attention_mask'][0]\n","\n","        if not self.is_test:\n","            return input_ids, attention_mask, self.df.loc[index, 'label']\n","        else:\n","            return input_ids, attention_mask\n","    \n","    def preprocess(self, text):\n","        text = text.replace('\\n', '')\n","        # text = text.replace('\\n', '</s>')\n","        # text = re.sub(r'https?://t.co/[a-zA-Z0-9]+', '', text)\n","\n","        return text"],"metadata":{"id":"3hfk3e8nOmjk","executionInfo":{"status":"ok","timestamp":1651997853938,"user_tz":-600,"elapsed":22,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class RumourDetector(nn.Module):\n","    def __init__(self, bert=bert):\n","        super(RumourDetector, self).__init__()\n","        self.bert_block = bert\n","        self.hidden_size = BertConfig.from_pretrained(bert_model).hidden_size\n","\n","        # 0.918\n","        # self.clf_block = nn.Sequential(\n","        #     nn.Dropout(0.7),\n","        #     nn.Linear(self.hidden_size, 1),\n","        #     nn.Sigmoid(),\n","        # )\n","\n","        # 0.93\n","        # self.clf_block = nn.Sequential(\n","        #     nn.Linear(self.hidden_size, self.hidden_size),\n","        #     nn.Dropout(0.5),\n","        #     nn.Linear(self.hidden_size, 256),\n","        #     nn.Linear(256, 128),\n","        #     nn.Linear(128, 1),\n","        #     nn.Sigmoid(),\n","        # )\n","\n","        self.clf_block = nn.Sequential(\n","            nn.Linear(self.hidden_size, self.hidden_size),\n","            nn.Dropout(0.7),\n","            nn.Linear(self.hidden_size, 256),\n","            nn.Linear(256, 128),\n","            nn.Linear(128, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, tweets_seqs, attn_masks):\n","        cls_reps = self.bert_block(tweets_seqs, attention_mask=attn_masks).last_hidden_state[:, 0, :]\n","\n","        # means = cls_reps.mean(dim=1, keepdim=True)\n","        # stds = cls_reps.std(dim=1, keepdim=True)\n","        # cls_reps = (cls_reps - means) / stds\n","\n","        probs = self.clf_block(cls_reps)\n","\n","        preds = (probs > 0.5).int()\n","\n","        del tweets_seqs, cls_reps\n","        torch.cuda.empty_cache()\n","\n","        return probs.flatten(), preds.flatten()\n"],"metadata":{"id":"WFKfPyauq81q","executionInfo":{"status":"ok","timestamp":1651997853940,"user_tz":-600,"elapsed":21,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def train(train_status, model, optim, epoch_size, train_loader, valid_loader):\n","    loss_fn = nn.BCELoss()\n","    \n","    # max_valid_f1 = 0\n","    for epoch in range(epoch_size):\n","        model.train()\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        train_loop = tqdm(enumerate(train_loader), total=len(train_loader))\n","        train_loop.set_description(f\"Epoch [{epoch+1}/{epoch_size}]\")\n","\n","        for batch, (tweets_seqs, attention_masks, labels) in train_loop:\n","            tweets_seqs = tweets_seqs.to(device)\n","            attention_masks = attention_masks.to(device)\n","            labels = labels.float().to(device)\n","            probs, preds = model(tweets_seqs, attention_masks)\n","            loss = F.binary_cross_entropy(probs, labels)\n","\n","            optim.zero_grad()\n","            loss.backward()\n","            optim.step() \n","\n","            epoch_loss += loss.item()\n","            epoch_acc += (preds == labels).float().mean().item()\n","            train_loop.set_postfix_str(\n","                'train_loss={:.5f}, train_acc={:.5f}'.format(\n","                    epoch_loss/(batch+1), epoch_acc/(batch+1)\n","                )\n","            )\n","\n","            del tweets_seqs, attention_masks, labels\n","            torch.cuda.empty_cache()\n","        \n","            if batch == len(train_loader)-1:\n","                valid_acc, valid_f1 = validate(model, valid_loader)\n","                # if valid_f1 > max_valid_f1:\n","                    # max_valid_f1 = valid_f1\n","                train_status['checkpoint'][epoch] = copy.deepcopy(model.state_dict())\n","                train_status['valid_acc'].append(valid_acc)\n","                train_status['valid_f1'].append(valid_f1)\n","                train_status['train_loss'].append(epoch_loss/(batch+1))\n","                train_status['train_acc'].append(epoch_acc/(batch+1))\n","                train_loop.set_postfix_str(\n","                    'train_loss={:.5f}, train_acc={:.5f}, valid_acc={:.5f}, valid_f1={:.5f}'.format(\n","                        train_status['train_loss'][-1],\n","                        train_status['train_acc'][-1],\n","                        train_status['valid_acc'][-1],\n","                        train_status['valid_f1'][-1]\n","                    )\n","                )\n","\n","    train_status['checkpoint']['train_status'] = train_status\n","\n","def validate(model, valid_loader):\n","    model.eval()\n","    acc = 0\n","    tp, fp, fn = 0, 0, 0\n","    with torch.no_grad():\n","        for batch, (inputs, attention_masks, labels) in enumerate(valid_loader):\n","            inputs = inputs.to(device)\n","            attention_masks = attention_masks.to(device)\n","            labels = labels.int().to(device)\n","            _, preds = model(inputs, attention_masks)\n","            \n","\n","            confusion_vector = preds / labels\n","            tp += torch.sum(confusion_vector == 1).item()\n","            fp += torch.sum(confusion_vector == float('inf')).item()\n","            fn += torch.sum(confusion_vector == 0).item()\n","\n","            acc += (preds == labels).float().mean()\n","            del inputs, attention_masks, labels, preds\n","            torch.cuda.empty_cache()\n","\n","        if (tp + fp == 0):\n","            precision = 0\n","        else:\n","            precision = tp / (tp + fp)\n","        \n","        if (tp + fn == 0):\n","            recall = 0\n","        else:\n","            recall = tp / (tp + fn)\n","        \n","        if (precision + recall == 0):\n","            f1 = 0\n","        else:\n","            f1 = (2 * precision * recall) / (precision + recall)\n","\n","    return acc / len(valid_loader), f1"],"metadata":{"id":"1aMsEQnaE2AN","executionInfo":{"status":"ok","timestamp":1651997853942,"user_tz":-600,"elapsed":21,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/') "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDOLvXxNNtn1","executionInfo":{"status":"ok","timestamp":1651997856383,"user_tz":-600,"elapsed":2461,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}},"outputId":"67168574-bf11-4ffd-a269-152d8710d857"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["epoch_size = 20\n","batch_size = 4\n","lr = 2e-5\n","\n","train_set = TweetDataset('/content/gdrive/MyDrive/data/train.csv')\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","valid_set = TweetDataset('/content/gdrive/MyDrive/data/dev.csv')\n","valid_loader = DataLoader(valid_set, batch_size=1, shuffle=True, num_workers=0)"],"metadata":{"id":"2Z2lYOoNPvCB","executionInfo":{"status":"ok","timestamp":1651997856385,"user_tz":-600,"elapsed":14,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = RumourDetector().to(device)\n","optim = torch.optim.AdamW(model.parameters(), lr=lr)\n","train_status = {'train_loss': [], 'train_acc': [], 'valid_acc': [], 'valid_f1': [], \n","                    'checkpoint': {}}\n","train(train_status, model, optim, epoch_size, train_loader, valid_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XWSy_nOQ0zX","outputId":"9fd5524a-3376-4d6f-be16-4416a451570c","executionInfo":{"status":"ok","timestamp":1651999041313,"user_tz":-600,"elapsed":1184940,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Epoch [1/20]: 100%|██████████| 452/452 [01:02<00:00,  7.25it/s, train_loss=0.51240, train_acc=0.76825, valid_acc=0.77250, valid_f1=0.00000]\n","Epoch [2/20]: 100%|██████████| 452/452 [01:01<00:00,  7.33it/s, train_loss=0.42994, train_acc=0.79701, valid_acc=0.83701, valid_f1=0.52941]\n","Epoch [3/20]: 100%|██████████| 452/452 [00:58<00:00,  7.79it/s, train_loss=0.28473, train_acc=0.88993, valid_acc=0.88285, valid_f1=0.67606]\n","Epoch [4/20]: 100%|██████████| 452/452 [00:58<00:00,  7.75it/s, train_loss=0.16148, train_acc=0.94414, valid_acc=0.92699, valid_f1=0.84249]\n","Epoch [5/20]: 100%|██████████| 452/452 [00:58<00:00,  7.71it/s, train_loss=0.10222, train_acc=0.96958, valid_acc=0.90662, valid_f1=0.76190]\n","Epoch [6/20]: 100%|██████████| 452/452 [00:58<00:00,  7.74it/s, train_loss=0.08710, train_acc=0.97345, valid_acc=0.92020, valid_f1=0.82528]\n","Epoch [7/20]: 100%|██████████| 452/452 [00:58<00:00,  7.75it/s, train_loss=0.05529, train_acc=0.98562, valid_acc=0.93548, valid_f1=0.85714]\n","Epoch [8/20]: 100%|██████████| 452/452 [00:58<00:00,  7.74it/s, train_loss=0.02803, train_acc=0.99115, valid_acc=0.94567, valid_f1=0.87970]\n","Epoch [9/20]: 100%|██████████| 452/452 [00:58<00:00,  7.68it/s, train_loss=0.00813, train_acc=0.99889, valid_acc=0.92530, valid_f1=0.83582]\n","Epoch [10/20]: 100%|██████████| 452/452 [00:58<00:00,  7.69it/s, train_loss=0.02811, train_acc=0.99170, valid_acc=0.93548, valid_f1=0.85714]\n","Epoch [11/20]: 100%|██████████| 452/452 [00:58<00:00,  7.73it/s, train_loss=0.03326, train_acc=0.99060, valid_acc=0.94058, valid_f1=0.87544]\n","Epoch [12/20]: 100%|██████████| 452/452 [00:58<00:00,  7.68it/s, train_loss=0.00901, train_acc=0.99723, valid_acc=0.92699, valid_f1=0.82158]\n","Epoch [13/20]: 100%|██████████| 452/452 [00:58<00:00,  7.70it/s, train_loss=0.00056, train_acc=1.00000, valid_acc=0.94058, valid_f1=0.87179]\n","Epoch [14/20]: 100%|██████████| 452/452 [00:58<00:00,  7.72it/s, train_loss=0.00025, train_acc=1.00000, valid_acc=0.94058, valid_f1=0.87179]\n","Epoch [15/20]: 100%|██████████| 452/452 [00:58<00:00,  7.75it/s, train_loss=0.00012, train_acc=1.00000, valid_acc=0.94397, valid_f1=0.87547]\n","Epoch [16/20]: 100%|██████████| 452/452 [00:58<00:00,  7.75it/s, train_loss=0.00009, train_acc=1.00000, valid_acc=0.94058, valid_f1=0.86590]\n","Epoch [17/20]: 100%|██████████| 452/452 [00:58<00:00,  7.76it/s, train_loss=0.00005, train_acc=1.00000, valid_acc=0.94058, valid_f1=0.86590]\n","Epoch [18/20]: 100%|██████████| 452/452 [00:59<00:00,  7.66it/s, train_loss=0.02563, train_acc=0.99392, valid_acc=0.92190, valid_f1=0.81148]\n","Epoch [19/20]: 100%|██████████| 452/452 [00:58<00:00,  7.68it/s, train_loss=0.06582, train_acc=0.98230, valid_acc=0.91851, valid_f1=0.81102]\n","Epoch [20/20]: 100%|██████████| 452/452 [00:59<00:00,  7.63it/s, train_loss=0.00464, train_acc=0.99834, valid_acc=0.91681, valid_f1=0.82182]\n"]}]},{"cell_type":"code","source":["torch.save(train_status['checkpoint'][12], '/content/gdrive/MyDrive/model/tweet_bert_mlp_clf.pt')\n","# torch.save(train_status['checkpoint'], '/content/gdrive/MyDrive/model/train_status.pt')"],"metadata":{"id":"RIWKFm0hbOdR","executionInfo":{"status":"ok","timestamp":1651999042921,"user_tz":-600,"elapsed":1636,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model = RumourDetector()\n","model.load_state_dict(torch.load('/content/gdrive/MyDrive/model/tweet_bert_mlp_clf.pt'))\n","model.to(device)"],"metadata":{"id":"8NeqBkDx5J01","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651999044005,"user_tz":-600,"elapsed":1107,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}},"outputId":"20e08a0e-d355-415d-fb8e-a301c716b971"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"execute_result","data":{"text/plain":["RumourDetector(\n","  (bert_block): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(130, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (clf_block): Sequential(\n","    (0): Linear(in_features=768, out_features=768, bias=True)\n","    (1): Dropout(p=0.7, inplace=False)\n","    (2): Linear(in_features=768, out_features=256, bias=True)\n","    (3): Linear(in_features=256, out_features=128, bias=True)\n","    (4): Linear(in_features=128, out_features=1, bias=True)\n","    (5): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["def test(model, test_loader):\n","    model.eval()\n","    labels = []\n","    with torch.no_grad():\n","        for batch, (inputs, attention_masks) in enumerate(test_loader):\n","            inputs = inputs.to(device)\n","            attention_masks = attention_masks.to(device)\n","            _, preds = model(inputs, attention_masks)\n","            preds = preds.tolist()\n","            labels.extend(preds)\n","            del inputs, attention_masks, preds\n","            torch.cuda.empty_cache()\n","    df = pd.DataFrame({'Id': list(range(0, len(test_loader))), 'Predicted': labels})\n","    df.to_csv('/content/gdrive/MyDrive/data/test.pred.csv', sep=',', index=False, encoding='utf-8')"],"metadata":{"id":"5Z7GG2mjcKJF","executionInfo":{"status":"ok","timestamp":1651999044008,"user_tz":-600,"elapsed":17,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["test_set = TweetDataset('/content/gdrive/MyDrive/data/test.csv', is_test=True)\n","test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n","test(model, test_loader=test_loader)"],"metadata":{"id":"73D627q0cVrr","executionInfo":{"status":"ok","timestamp":1651999050351,"user_tz":-600,"elapsed":6357,"user":{"displayName":"s0coRrECT Ur","userId":"05768332687332134567"}}},"execution_count":16,"outputs":[]}]}